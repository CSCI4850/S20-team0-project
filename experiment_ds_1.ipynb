{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hgg_utils as hu\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm \n",
    "from model import unet\n",
    "from utils.dice import dice_loss as dice\n",
    "from utils.dice import dice_coef as dice_coef\n",
    "from sklearn.utils import shuffle\n",
    "from IPython import display\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy = mixed_precision.Policy(\"float32\") \n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_load = 5\n",
    "n_slices = 155\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The val that varies between experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to load in some input data and masks and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = hu.get_each_normalized_hgg_folder()\n",
    "patients = hu.remove_outliers(patients)\n",
    "\n",
    "masks = hu.get_each_hgg_folder()\n",
    "masks = hu.remove_outliers(masks)\n",
    "\n",
    "patients, masks = shuffle(patients, masks, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_start = 0\n",
    "train_stop = int(np.round(train_data_ratio * len(patients)))\n",
    "print(train_start)\n",
    "train_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_start = train_stop\n",
    "test_stop = len(patients)\n",
    "print(test_start)\n",
    "test_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save paths for train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = patients[:test_start]\n",
    "train_masks = masks[:test_start]\n",
    "\n",
    "fname_train_data = \"ds_\"+str(ds)+\"_train_data.pkl\"\n",
    "fname_train_masks = \"ds_\"+str(ds)+\"_train_masks.pkl\"\n",
    "\n",
    "with open(fname_train_data, 'wb') as file_pi:\n",
    "    pickle.dump(train_data, file_pi)\n",
    "    \n",
    "with open(fname_train_masks, 'wb') as file_pi:\n",
    "    pickle.dump(train_masks, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the paths for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = patients[test_start:]\n",
    "test_masks = masks[test_start:]\n",
    "\n",
    "fname_test_data = \"ds_\"+str(ds)+\"_test_data.pkl\"\n",
    "fname_test_masks = \"ds_\"+str(ds)+\"_test_masks.pkl\"\n",
    "\n",
    "with open(fname_test_data, 'wb') as file_pi:\n",
    "    pickle.dump(test_data, file_pi)\n",
    "    \n",
    "with open(fname_test_masks, 'wb') as file_pi:\n",
    "    pickle.dump(test_masks, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preallocate arrays to hold data & masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = np.ones([ num_to_load*155, 240, 240, 4])\n",
    "some_masks = np.ones([ num_to_load*155, 240, 240, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_brains(data, start, stop, paths, end):\n",
    "\n",
    "    data_idx = 0\n",
    "    num_slices = 155\n",
    "    brains_seen = 0\n",
    "    \n",
    "    #for multimodal_tensor in tqdm(range(start, stop)):\n",
    "    for multimodal_tensor in range(start, stop):\n",
    "\n",
    "        if multimodal_tensor != end:\n",
    "            four_channel_scan = hu.reshape_tensor_with_slices_first(\n",
    "                                    hu.get_a_multimodal_tensor( \n",
    "                                                paths[multimodal_tensor] \n",
    "                                    )[data_idx]\n",
    "            )\n",
    "            #print(paths[multimodal_tensor])\n",
    "\n",
    "\n",
    "            for slic in range(num_slices):\n",
    "                data[slic+(num_slices*brains_seen),:,:,:] = four_channel_scan[slic,:,:,:]\n",
    "\n",
    "            brains_seen += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    #print(multimodal_tensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_masks(data, start, stop, paths, end):\n",
    "\n",
    "    data_idx = 0\n",
    "    num_slices = 155\n",
    "    brains_seen = 0\n",
    "\n",
    "    for mask_idx in range(start, stop):\n",
    "        if mask_idx != end:\n",
    "\n",
    "            mask =  hu.reshape_tensor_with_slices_first(\n",
    "                                    hu.convert_mask_to_binary_mask(\n",
    "                                         hu.get_a_mask_tensor( paths[mask_idx] )\n",
    "\n",
    "                                   )\n",
    "            )\n",
    "            #print(paths[mask_idx])\n",
    "\n",
    "\n",
    "            for slic in range(num_slices):\n",
    "                data[slic+(num_slices*brains_seen),:,:,:] = mask[slic,:,:,:]\n",
    "\n",
    "            brains_seen += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 39\n",
    "\n",
    "beg = 0\n",
    "end = num_to_load*155\n",
    "truncated = 155*(train_stop - ( (chunks-1) * num_to_load  ) )\n",
    "\n",
    "#truncated\n",
    "#155*(train_stop - ( (chunks-1) * num_to_load  ) )\n",
    "\n",
    "my_opt = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd826fb8d9243d9996035639d5b01e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Run: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b462b574e18c42b386163be7cfcae114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 45s 36ms/sample - loss: 0.9648 - dice_coef: 0.0352 - val_loss: 0.9640 - val_dice_coef: 0.0358\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9603 - dice_coef: 0.0398 - val_loss: 0.9570 - val_dice_coef: 0.0432\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9666 - dice_coef: 0.0335 - val_loss: 0.9675 - val_dice_coef: 0.0328\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9506 - dice_coef: 0.0494 - val_loss: 0.9566 - val_dice_coef: 0.0428\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9652 - dice_coef: 0.0348 - val_loss: 0.9688 - val_dice_coef: 0.0307\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9542 - dice_coef: 0.0458 - val_loss: 0.9532 - val_dice_coef: 0.0466\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9583 - dice_coef: 0.0417 - val_loss: 0.9578 - val_dice_coef: 0.0414\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9694 - dice_coef: 0.0305 - val_loss: 0.9711 - val_dice_coef: 0.0286\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 29s 23ms/sample - loss: 0.9742 - dice_coef: 0.0258 - val_loss: 0.9782 - val_dice_coef: 0.0214\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9580 - dice_coef: 0.0422 - val_loss: 0.9558 - val_dice_coef: 0.0440\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9566 - dice_coef: 0.0433 - val_loss: 0.9607 - val_dice_coef: 0.0390\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9600 - dice_coef: 0.0400 - val_loss: 0.9631 - val_dice_coef: 0.0362\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9656 - dice_coef: 0.0343 - val_loss: 0.9632 - val_dice_coef: 0.0370\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9651 - dice_coef: 0.0348 - val_loss: 0.9640 - val_dice_coef: 0.0358\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9530 - dice_coef: 0.0470 - val_loss: 0.9541 - val_dice_coef: 0.0457\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9581 - dice_coef: 0.0419 - val_loss: 0.9570 - val_dice_coef: 0.0422\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9600 - dice_coef: 0.0399 - val_loss: 0.9566 - val_dice_coef: 0.0435\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9603 - dice_coef: 0.0396 - val_loss: 0.9631 - val_dice_coef: 0.0366\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9737 - dice_coef: 0.0263 - val_loss: 0.9663 - val_dice_coef: 0.0340\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 17s 35ms/sample - loss: 0.9745 - dice_coef: 0.0253 - val_loss: 0.9779 - val_dice_coef: 0.0223\n",
      "Epoch 0 completed\n",
      "Elapsed time: 12.821666459242502 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9504 - dice_coef: 0.0498 - val_loss: 0.9582 - val_dice_coef: 0.0416\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9539 - dice_coef: 0.0460 - val_loss: 0.9529 - val_dice_coef: 0.0473\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9629 - dice_coef: 0.0369 - val_loss: 0.9645 - val_dice_coef: 0.0358\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 28s 23ms/sample - loss: 0.9463 - dice_coef: 0.0537 - val_loss: 0.9527 - val_dice_coef: 0.0466\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 21ms/sample - loss: 0.9619 - dice_coef: 0.0380 - val_loss: 0.9661 - val_dice_coef: 0.0334\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9506 - dice_coef: 0.0493 - val_loss: 0.9515 - val_dice_coef: 0.0481\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9576 - dice_coef: 0.0426 - val_loss: 0.9559 - val_dice_coef: 0.0432\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9680 - dice_coef: 0.0319 - val_loss: 0.9696 - val_dice_coef: 0.0301\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9732 - dice_coef: 0.0267 - val_loss: 0.9773 - val_dice_coef: 0.0223\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9560 - dice_coef: 0.0439 - val_loss: 0.9533 - val_dice_coef: 0.0465\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9549 - dice_coef: 0.0453 - val_loss: 0.9590 - val_dice_coef: 0.0407\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9581 - dice_coef: 0.0419 - val_loss: 0.9614 - val_dice_coef: 0.0379\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9621 - dice_coef: 0.0379 - val_loss: 0.9600 - val_dice_coef: 0.0401\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9626 - dice_coef: 0.0374 - val_loss: 0.9616 - val_dice_coef: 0.0383\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9502 - dice_coef: 0.0498 - val_loss: 0.9519 - val_dice_coef: 0.0478\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9562 - dice_coef: 0.0439 - val_loss: 0.9550 - val_dice_coef: 0.0442\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9578 - dice_coef: 0.0422 - val_loss: 0.9543 - val_dice_coef: 0.0458\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9583 - dice_coef: 0.0417 - val_loss: 0.9614 - val_dice_coef: 0.0383\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9723 - dice_coef: 0.0277 - val_loss: 0.9649 - val_dice_coef: 0.0354\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 21ms/sample - loss: 0.9733 - dice_coef: 0.0273 - val_loss: 0.9765 - val_dice_coef: 0.0237\n",
      "Epoch 1 completed\n",
      "Elapsed time: 24.5878977338473 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9482 - dice_coef: 0.0519 - val_loss: 0.9564 - val_dice_coef: 0.0433\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9519 - dice_coef: 0.0480 - val_loss: 0.9510 - val_dice_coef: 0.0492\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 21ms/sample - loss: 0.9613 - dice_coef: 0.0386 - val_loss: 0.9630 - val_dice_coef: 0.0373\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9441 - dice_coef: 0.0558 - val_loss: 0.9506 - val_dice_coef: 0.0487\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9611 - dice_coef: 0.0388 - val_loss: 0.9650 - val_dice_coef: 0.0345\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9487 - dice_coef: 0.0514 - val_loss: 0.9495 - val_dice_coef: 0.0500\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9529 - dice_coef: 0.0471 - val_loss: 0.9525 - val_dice_coef: 0.0465\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9656 - dice_coef: 0.0344 - val_loss: 0.9679 - val_dice_coef: 0.0318\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9718 - dice_coef: 0.0281 - val_loss: 0.9762 - val_dice_coef: 0.0233\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9535 - dice_coef: 0.0465 - val_loss: 0.9514 - val_dice_coef: 0.0484\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9524 - dice_coef: 0.0476 - val_loss: 0.9570 - val_dice_coef: 0.0427\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9557 - dice_coef: 0.0443 - val_loss: 0.9596 - val_dice_coef: 0.0397\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9603 - dice_coef: 0.0396 - val_loss: 0.9593 - val_dice_coef: 0.0409\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9611 - dice_coef: 0.0387 - val_loss: 0.9600 - val_dice_coef: 0.0399\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9482 - dice_coef: 0.0517 - val_loss: 0.9498 - val_dice_coef: 0.0499\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9539 - dice_coef: 0.0461 - val_loss: 0.9528 - val_dice_coef: 0.0463\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9554 - dice_coef: 0.0446 - val_loss: 0.9521 - val_dice_coef: 0.0480\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9565 - dice_coef: 0.0435 - val_loss: 0.9597 - val_dice_coef: 0.0400\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9710 - dice_coef: 0.0289 - val_loss: 0.9633 - val_dice_coef: 0.0370\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 21ms/sample - loss: 0.9721 - dice_coef: 0.0274 - val_loss: 0.9755 - val_dice_coef: 0.0247\n",
      "Epoch 2 completed\n",
      "Elapsed time: 36.02400345007579 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9459 - dice_coef: 0.0541 - val_loss: 0.9545 - val_dice_coef: 0.0452\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9499 - dice_coef: 0.0500 - val_loss: 0.9488 - val_dice_coef: 0.0514\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9597 - dice_coef: 0.0403 - val_loss: 0.9614 - val_dice_coef: 0.0389\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9417 - dice_coef: 0.0583 - val_loss: 0.9485 - val_dice_coef: 0.0508\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9584 - dice_coef: 0.0415 - val_loss: 0.9629 - val_dice_coef: 0.0366\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9466 - dice_coef: 0.0536 - val_loss: 0.9474 - val_dice_coef: 0.0521\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9509 - dice_coef: 0.0492 - val_loss: 0.9503 - val_dice_coef: 0.0487\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9639 - dice_coef: 0.0361 - val_loss: 0.9663 - val_dice_coef: 0.0333\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9703 - dice_coef: 0.0296 - val_loss: 0.9751 - val_dice_coef: 0.0245\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9513 - dice_coef: 0.0487 - val_loss: 0.9490 - val_dice_coef: 0.0508\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9502 - dice_coef: 0.0498 - val_loss: 0.9551 - val_dice_coef: 0.0446\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9538 - dice_coef: 0.0462 - val_loss: 0.9579 - val_dice_coef: 0.0413\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9580 - dice_coef: 0.0419 - val_loss: 0.9568 - val_dice_coef: 0.0434\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9592 - dice_coef: 0.0407 - val_loss: 0.9580 - val_dice_coef: 0.0418\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9458 - dice_coef: 0.0540 - val_loss: 0.9475 - val_dice_coef: 0.0522\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9517 - dice_coef: 0.0483 - val_loss: 0.9506 - val_dice_coef: 0.0485\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9536 - dice_coef: 0.0464 - val_loss: 0.9502 - val_dice_coef: 0.0498\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9545 - dice_coef: 0.0455 - val_loss: 0.9579 - val_dice_coef: 0.0418\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9698 - dice_coef: 0.0301 - val_loss: 0.9617 - val_dice_coef: 0.0386\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 21ms/sample - loss: 0.9707 - dice_coef: 0.0298 - val_loss: 0.9743 - val_dice_coef: 0.0260\n",
      "Epoch 3 completed\n",
      "Elapsed time: 47.55670081774394 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9433 - dice_coef: 0.0568 - val_loss: 0.9525 - val_dice_coef: 0.0473\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9476 - dice_coef: 0.0524 - val_loss: 0.9466 - val_dice_coef: 0.0537\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9579 - dice_coef: 0.0422 - val_loss: 0.9597 - val_dice_coef: 0.0406\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9391 - dice_coef: 0.0607 - val_loss: 0.9462 - val_dice_coef: 0.0531\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9565 - dice_coef: 0.0434 - val_loss: 0.9613 - val_dice_coef: 0.0381\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9451 - dice_coef: 0.0549 - val_loss: 0.9455 - val_dice_coef: 0.0540\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9496 - dice_coef: 0.0504 - val_loss: 0.9488 - val_dice_coef: 0.0501\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9626 - dice_coef: 0.0376 - val_loss: 0.9650 - val_dice_coef: 0.0347\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9690 - dice_coef: 0.0310 - val_loss: 0.9739 - val_dice_coef: 0.0256\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9496 - dice_coef: 0.0503 - val_loss: 0.9472 - val_dice_coef: 0.0526\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9484 - dice_coef: 0.0515 - val_loss: 0.9533 - val_dice_coef: 0.0464\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9517 - dice_coef: 0.0482 - val_loss: 0.9562 - val_dice_coef: 0.0430\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9566 - dice_coef: 0.0435 - val_loss: 0.9544 - val_dice_coef: 0.0458\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9577 - dice_coef: 0.0422 - val_loss: 0.9563 - val_dice_coef: 0.0436\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9433 - dice_coef: 0.0567 - val_loss: 0.9454 - val_dice_coef: 0.0543\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 21ms/sample - loss: 0.9497 - dice_coef: 0.0505 - val_loss: 0.9485 - val_dice_coef: 0.0505\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9519 - dice_coef: 0.0482 - val_loss: 0.9478 - val_dice_coef: 0.0523\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9526 - dice_coef: 0.0473 - val_loss: 0.9561 - val_dice_coef: 0.0436\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9685 - dice_coef: 0.0316 - val_loss: 0.9600 - val_dice_coef: 0.0403\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 20ms/sample - loss: 0.9695 - dice_coef: 0.0302 - val_loss: 0.9731 - val_dice_coef: 0.0271\n",
      "Epoch 4 completed\n",
      "Elapsed time: 60.55728490352631 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9410 - dice_coef: 0.0591 - val_loss: 0.9505 - val_dice_coef: 0.0492\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9454 - dice_coef: 0.0546 - val_loss: 0.9443 - val_dice_coef: 0.0559\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9561 - dice_coef: 0.0439 - val_loss: 0.9578 - val_dice_coef: 0.0424\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9363 - dice_coef: 0.0637 - val_loss: 0.9438 - val_dice_coef: 0.0554\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9547 - dice_coef: 0.0454 - val_loss: 0.9595 - val_dice_coef: 0.0399\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9420 - dice_coef: 0.0582 - val_loss: 0.9430 - val_dice_coef: 0.0565\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9461 - dice_coef: 0.0541 - val_loss: 0.9457 - val_dice_coef: 0.0532\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9605 - dice_coef: 0.0394 - val_loss: 0.9632 - val_dice_coef: 0.0364\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9675 - dice_coef: 0.0326 - val_loss: 0.9727 - val_dice_coef: 0.0268\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9468 - dice_coef: 0.0534 - val_loss: 0.9442 - val_dice_coef: 0.0556\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9458 - dice_coef: 0.0541 - val_loss: 0.9511 - val_dice_coef: 0.0486\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9495 - dice_coef: 0.0505 - val_loss: 0.9540 - val_dice_coef: 0.0452\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9540 - dice_coef: 0.0459 - val_loss: 0.9522 - val_dice_coef: 0.0480\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9556 - dice_coef: 0.0445 - val_loss: 0.9542 - val_dice_coef: 0.0457\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9406 - dice_coef: 0.0592 - val_loss: 0.9427 - val_dice_coef: 0.0570\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9472 - dice_coef: 0.0529 - val_loss: 0.9460 - val_dice_coef: 0.0530\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9490 - dice_coef: 0.0510 - val_loss: 0.9455 - val_dice_coef: 0.0546\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9504 - dice_coef: 0.0495 - val_loss: 0.9540 - val_dice_coef: 0.0457\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 26s 21ms/sample - loss: 0.9670 - dice_coef: 0.0332 - val_loss: 0.9581 - val_dice_coef: 0.0422\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 20ms/sample - loss: 0.9681 - dice_coef: 0.0326 - val_loss: 0.9719 - val_dice_coef: 0.0283\n",
      "Epoch 5 completed\n",
      "Elapsed time: 71.95824348131815 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9382 - dice_coef: 0.0617 - val_loss: 0.9481 - val_dice_coef: 0.0517\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9427 - dice_coef: 0.0573 - val_loss: 0.9416 - val_dice_coef: 0.0587\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9540 - dice_coef: 0.0458 - val_loss: 0.9558 - val_dice_coef: 0.0445\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9335 - dice_coef: 0.0665 - val_loss: 0.9413 - val_dice_coef: 0.0579\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9525 - dice_coef: 0.0477 - val_loss: 0.9576 - val_dice_coef: 0.0418\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9395 - dice_coef: 0.0605 - val_loss: 0.9405 - val_dice_coef: 0.0589\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9435 - dice_coef: 0.0566 - val_loss: 0.9431 - val_dice_coef: 0.0557\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9585 - dice_coef: 0.0415 - val_loss: 0.9614 - val_dice_coef: 0.0382\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9658 - dice_coef: 0.0342 - val_loss: 0.9714 - val_dice_coef: 0.0281\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9443 - dice_coef: 0.0560 - val_loss: 0.9416 - val_dice_coef: 0.0582\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9432 - dice_coef: 0.0567 - val_loss: 0.9487 - val_dice_coef: 0.0509\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9472 - dice_coef: 0.0529 - val_loss: 0.9517 - val_dice_coef: 0.0474\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9515 - dice_coef: 0.0484 - val_loss: 0.9500 - val_dice_coef: 0.0502\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9534 - dice_coef: 0.0466 - val_loss: 0.9519 - val_dice_coef: 0.0479\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9377 - dice_coef: 0.0624 - val_loss: 0.9401 - val_dice_coef: 0.0596\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9448 - dice_coef: 0.0553 - val_loss: 0.9435 - val_dice_coef: 0.0555\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9467 - dice_coef: 0.0531 - val_loss: 0.9431 - val_dice_coef: 0.0570\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9481 - dice_coef: 0.0520 - val_loss: 0.9518 - val_dice_coef: 0.0479\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9654 - dice_coef: 0.0346 - val_loss: 0.9561 - val_dice_coef: 0.0442\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 20ms/sample - loss: 0.9666 - dice_coef: 0.0327 - val_loss: 0.9704 - val_dice_coef: 0.0298\n",
      "Epoch 6 completed\n",
      "Elapsed time: 83.45345012744268 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9353 - dice_coef: 0.0647 - val_loss: 0.9456 - val_dice_coef: 0.0541\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9401 - dice_coef: 0.0601 - val_loss: 0.9389 - val_dice_coef: 0.0614\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9518 - dice_coef: 0.0481 - val_loss: 0.9537 - val_dice_coef: 0.0467\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9302 - dice_coef: 0.0698 - val_loss: 0.9384 - val_dice_coef: 0.0608\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9501 - dice_coef: 0.0497 - val_loss: 0.9555 - val_dice_coef: 0.0438\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9368 - dice_coef: 0.0630 - val_loss: 0.9376 - val_dice_coef: 0.0618\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9409 - dice_coef: 0.0591 - val_loss: 0.9405 - val_dice_coef: 0.0583\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9566 - dice_coef: 0.0435 - val_loss: 0.9595 - val_dice_coef: 0.0400\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9641 - dice_coef: 0.0358 - val_loss: 0.9699 - val_dice_coef: 0.0295\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9417 - dice_coef: 0.0584 - val_loss: 0.9389 - val_dice_coef: 0.0609\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9406 - dice_coef: 0.0593 - val_loss: 0.9463 - val_dice_coef: 0.0534\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9445 - dice_coef: 0.0555 - val_loss: 0.9494 - val_dice_coef: 0.0497\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9492 - dice_coef: 0.0506 - val_loss: 0.9475 - val_dice_coef: 0.0527\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9509 - dice_coef: 0.0489 - val_loss: 0.9495 - val_dice_coef: 0.0503\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9345 - dice_coef: 0.0656 - val_loss: 0.9371 - val_dice_coef: 0.0625\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9420 - dice_coef: 0.0579 - val_loss: 0.9408 - val_dice_coef: 0.0581\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9440 - dice_coef: 0.0559 - val_loss: 0.9399 - val_dice_coef: 0.0602\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 21ms/sample - loss: 0.9453 - dice_coef: 0.0546 - val_loss: 0.9494 - val_dice_coef: 0.0502\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 22ms/sample - loss: 0.9636 - dice_coef: 0.0363 - val_loss: 0.9539 - val_dice_coef: 0.0464\n",
      "Loading chunk of data...\n",
      "Train on 496 samples, validate on 124 samples\n",
      "496/496 [==============================] - 10s 20ms/sample - loss: 0.9648 - dice_coef: 0.0349 - val_loss: 0.9689 - val_dice_coef: 0.0313\n",
      "Epoch 7 completed\n",
      "Elapsed time: 95.02101111809412 minutes\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9320 - dice_coef: 0.0680 - val_loss: 0.9429 - val_dice_coef: 0.0568\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9372 - dice_coef: 0.0627 - val_loss: 0.9360 - val_dice_coef: 0.0643\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9495 - dice_coef: 0.0506 - val_loss: 0.9515 - val_dice_coef: 0.0489\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9268 - dice_coef: 0.0733 - val_loss: 0.9354 - val_dice_coef: 0.0637\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9478 - dice_coef: 0.0521 - val_loss: 0.9535 - val_dice_coef: 0.0458\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9339 - dice_coef: 0.0660 - val_loss: 0.9348 - val_dice_coef: 0.0646\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9380 - dice_coef: 0.0619 - val_loss: 0.9377 - val_dice_coef: 0.0610\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9543 - dice_coef: 0.0457 - val_loss: 0.9575 - val_dice_coef: 0.0421\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9623 - dice_coef: 0.0376 - val_loss: 0.9684 - val_dice_coef: 0.0311\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9389 - dice_coef: 0.0610 - val_loss: 0.9364 - val_dice_coef: 0.0633\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 27s 21ms/sample - loss: 0.9375 - dice_coef: 0.0625 - val_loss: 0.9436 - val_dice_coef: 0.0560\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9420 - dice_coef: 0.0579 - val_loss: 0.9470 - val_dice_coef: 0.0520\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9470 - dice_coef: 0.0530 - val_loss: 0.9448 - val_dice_coef: 0.0554\n",
      "Loading chunk of data...\n",
      "Train on 1240 samples, validate on 310 samples\n",
      "1240/1240 [==============================] - 25s 20ms/sample - loss: 0.9485 - dice_coef: 0.0515 - val_loss: 0.9471 - val_dice_coef: 0.0527\n",
      "Loading chunk of data...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.33 GiB for an array with shape (1550, 240, 240, 4) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-461305bbb623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0msome_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_n_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0msome_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msome_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msome_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msome_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnum_to_load\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum_to_load\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mtrain_stop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\4850\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \"\"\"\n\u001b[0;32m    700\u001b[0m     \u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\4850\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m     \u001b[0mresampled_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\4850\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;31m# convert sparse matrices to CSR for row-based indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m     \u001b[0mresampled_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresampled_arrays\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[1;31m# syntactic sugar for the unit argument case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\4850\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\4850\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.33 GiB for an array with shape (1550, 240, 240, 4) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for run in tqdm(range(5)):\n",
    "    \n",
    "    print(\"********************\")\n",
    "    print(\"Run:\", run)\n",
    "    \n",
    "    model = unet( input_size=(240,240,4), ds=ds )\n",
    "    # Save architecture\n",
    "    model_json_name = \"unet_ds_{}.json\".format(ds)\n",
    "    with open(model_json_name, \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "        \n",
    "    model.compile(optimizer=my_opt, loss=dice, metrics=[dice_coef])\n",
    "    \n",
    "    run_history = []\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        \n",
    "        epoch_history = []\n",
    "        \n",
    "        for i in range(chunks):\n",
    "            print(\"Loading chunk of data...\")\n",
    "            some_data = load_n_brains(some_data,  (num_to_load*i), (num_to_load*i)+num_to_load, patients, train_stop).astype(np.float32)\n",
    "            some_masks = load_n_masks(some_masks, (num_to_load*i), (num_to_load*i)+num_to_load, masks, train_stop).astype(np.float32)\n",
    "\n",
    "            some_data, some_masks = shuffle(some_data, some_masks, random_state=1)\n",
    "\n",
    "            if num_to_load*i+num_to_load <= train_stop:\n",
    "                history = model.fit(some_data[beg:end,...], some_masks[beg:end,...], validation_split=0.2, epochs=1, batch_size=32)\n",
    "\n",
    "            else:\n",
    "                history = model.fit(some_data[beg:truncated,...], some_masks[beg:truncated,...], validation_split=0.2, epochs=1, batch_size=32)\n",
    "\n",
    "\n",
    "            epoch_history.append(history.history)\n",
    "        print(\"Epoch\", epoch, \"completed\")\n",
    "        print(\"Elapsed time:\", (time.time() - start_time)/60.0, \"minutes\" )\n",
    "        run_history.append(epoch_history)\n",
    "    \n",
    "    print()\n",
    "    print(\"Saving run\", run, \"loss etc.\")\n",
    "    history_name = \"ds_\"+str(ds)+\"_run_\" + str(run) +\"_histories.pkl\"\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(run_history, file_pi)\n",
    "    \n",
    "    model_weights_name = \"ds_\"+str(ds)+\"_run_\" + str(run) +\"_model_weights.h5\"\n",
    "    \n",
    "    print(\"Saving run\", run, \"model weights as\", model_weights_name)\n",
    "    model.save_weights(model_weights_name)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "print(\"Total time:\", (time.time() - start_time)/60.0, \"minutes\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print( run_history[0].history )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
