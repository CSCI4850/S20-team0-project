{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hgg_utils as hu\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm \n",
    "from model import unet\n",
    "from utils.dice import dice_loss as dice\n",
    "from utils.dice import dice_coef as dice_coef\n",
    "from sklearn.utils import shuffle\n",
    "from IPython import display\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import time\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy = mixed_precision.Policy(\"float32\") \n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_load = 5\n",
    "n_slices = 155\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The val that varies between experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to load in some input data and masks and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = hu.get_each_normalized_hgg_folder()\n",
    "patients = hu.remove_outliers(patients)\n",
    "\n",
    "masks = hu.get_each_hgg_folder()\n",
    "masks = hu.remove_outliers(masks)\n",
    "\n",
    "patients, masks = shuffle(patients, masks, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_start = 0\n",
    "train_stop = int(np.round(train_data_ratio * len(patients)))\n",
    "print(train_start)\n",
    "train_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_start = train_stop\n",
    "test_stop = len(patients)\n",
    "print(test_start)\n",
    "test_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save paths for train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = patients[:test_start]\n",
    "train_masks = masks[:test_start]\n",
    "\n",
    "fname_train_data = \"ds_\"+str(ds)+\"_train_data.pkl\"\n",
    "fname_train_masks = \"ds_\"+str(ds)+\"_train_masks.pkl\"\n",
    "\n",
    "with open(fname_train_data, 'wb') as file_pi:\n",
    "    pickle.dump(train_data, file_pi)\n",
    "    \n",
    "with open(fname_train_masks, 'wb') as file_pi:\n",
    "    pickle.dump(train_masks, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the paths for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = patients[test_start:]\n",
    "test_masks = masks[test_start:]\n",
    "\n",
    "fname_test_data = \"ds_\"+str(ds)+\"_test_data.pkl\"\n",
    "fname_test_masks = \"ds_\"+str(ds)+\"_test_masks.pkl\"\n",
    "\n",
    "with open(fname_test_data, 'wb') as file_pi:\n",
    "    pickle.dump(test_data, file_pi)\n",
    "    \n",
    "with open(fname_test_masks, 'wb') as file_pi:\n",
    "    pickle.dump(test_masks, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preallocate arrays to hold data & masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = np.ones([ num_to_load*155, 240, 240, 4])\n",
    "some_masks = np.ones([ num_to_load*155, 240, 240, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_brains(data, start, stop, paths, end):\n",
    "\n",
    "    data_idx = 0\n",
    "    num_slices = 155\n",
    "    brains_seen = 0\n",
    "    \n",
    "    #for multimodal_tensor in tqdm(range(start, stop)):\n",
    "    for multimodal_tensor in range(start, stop):\n",
    "\n",
    "        if multimodal_tensor != end:\n",
    "            four_channel_scan = hu.reshape_tensor_with_slices_first(\n",
    "                                    hu.get_a_multimodal_tensor( \n",
    "                                                paths[multimodal_tensor] \n",
    "                                    )[data_idx]\n",
    "            )\n",
    "            #print(paths[multimodal_tensor])\n",
    "\n",
    "\n",
    "            for slic in range(num_slices):\n",
    "                data[slic+(num_slices*brains_seen),:,:,:] = four_channel_scan[slic,:,:,:]\n",
    "\n",
    "            brains_seen += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    #print(multimodal_tensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_masks(data, start, stop, paths, end):\n",
    "\n",
    "    data_idx = 0\n",
    "    num_slices = 155\n",
    "    brains_seen = 0\n",
    "\n",
    "    for mask_idx in range(start, stop):\n",
    "        if mask_idx != end:\n",
    "\n",
    "            mask =  hu.reshape_tensor_with_slices_first(\n",
    "                                    hu.convert_mask_to_binary_mask(\n",
    "                                         hu.get_a_mask_tensor( paths[mask_idx] )\n",
    "\n",
    "                                   )\n",
    "            )\n",
    "            #print(paths[mask_idx])\n",
    "\n",
    "\n",
    "            for slic in range(num_slices):\n",
    "                data[slic+(num_slices*brains_seen),:,:,:] = mask[slic,:,:,:]\n",
    "\n",
    "            brains_seen += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 39\n",
    "\n",
    "beg = 0\n",
    "end = num_to_load*155\n",
    "truncated = 155*(train_stop - ( (chunks-1) * num_to_load  ) )\n",
    "\n",
    "#truncated\n",
    "#155*(train_stop - ( (chunks-1) * num_to_load  ) )\n",
    "\n",
    "my_opt = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013e8bd9316d467a92331382ffcd7fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Run: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbe5b6f806843e388472abb3c9a89ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 14s 6ms/sample - loss: 0.9769 - dice_coef: 0.0232 - val_loss: 0.9715 - val_dice_coef: 0.0280s: 0.9771\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 9s 4ms/sample - loss: 0.9651 - dice_coef: 0.0349 - val_loss: 0.9587 - val_dice_coef: 0.041334\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 11s 4ms/sample - loss: 0.9604 - dice_coef: 0.0398 - val_loss: 0.9598 - val_dice_coef: 0.0391TA: 4s -\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 11s 4ms/sample - loss: 0.9647 - dice_coef: 0.0353 - val_loss: 0.9629 - val_dice_coef: 0.0366: \n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 11s 4ms/sample - loss: 0.9510 - dice_coef: 0.0518 - val_loss: 0.5480 - val_dice_coef: 0.45150.03 - ETA: 0s - loss: 0.9637 - dice_coef: \n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 9s 4ms/sample - loss: 0.3122 - dice_coef: 0.6885 - val_loss: 0.2429 - val_dice_coef: 0.7603coef: 0 - ETA: 1s - loss: 0.3252 - dice_ - ETA: 0s - loss: 0.3161 - dice_coef: 0.\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 9s 4ms/sample - loss: 0.2994 - dice_coef: 0.7001 - val_loss: 0.2930 - val_dice_coef: 0.7080- dic - ETA: 3s - loss: 0.3200 - dice_coe - ETA: 2s - loss: 0.3101 - dice_ - ETA: 1s - loss: 0.3004 - dice_coef: 0 - ETA: 0s - loss: 0.3007 - dice_coef\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "2480/2480 [==============================] - 11s 4ms/sample - loss: 0.2476 - dice_coef: 0.7526 - val_loss: 0.2380 - val_dice_coef: 0.7611\n",
      "Loading chunk of data...\n",
      "Train on 2480 samples, validate on 620 samples\n",
      "1824/2480 [=====================>........] - ETA: 2s - loss: 0.2835 - dice_coef: 0.7165- ETA: 4s - loss: 0.2926 - dice_coef: 0.707 - ETA: 4s - loss: 0.2"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for run in tqdm(range(5)):\n",
    "    \n",
    "    print(\"********************\")\n",
    "    print(\"Run:\", run)\n",
    "    \n",
    "    model = unet( input_size=(240,240,4), ds=ds )\n",
    "    # Save architecture\n",
    "    model_json_name = \"unet_ds_{}.json\".format(ds)\n",
    "    with open(model_json_name, \"w\") as json_file:\n",
    "        json_file.write(model.to_json())\n",
    "        \n",
    "    model.compile(optimizer=my_opt, loss=dice, metrics=[dice_coef])\n",
    "    \n",
    "    run_history = []\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        \n",
    "        epoch_history = []\n",
    "        \n",
    "        for i in range(chunks):\n",
    "            print(\"Loading chunk of data...\")\n",
    "            some_data = load_n_brains(some_data,  (num_to_load*i), (num_to_load*i)+num_to_load, patients, train_stop).astype(np.float32)\n",
    "            some_masks = load_n_masks(some_masks, (num_to_load*i), (num_to_load*i)+num_to_load, masks, train_stop).astype(np.float32)\n",
    "\n",
    "            some_data, some_masks = shuffle(some_data, some_masks, random_state=1)\n",
    "\n",
    "            if num_to_load*i+num_to_load <= train_stop:\n",
    "                history = model.fit(some_data[beg:end,...], some_masks[beg:end,...], validation_split=0.2, epochs=1, batch_size=16, verbose=0)\n",
    "\n",
    "            else:\n",
    "                history = model.fit(some_data[beg:truncated,...], some_masks[beg:truncated,...], validation_split=0.2, epochs=1, batch_size=16, verbose=0)\n",
    "\n",
    "\n",
    "            epoch_history.append(history.history)\n",
    "            gc.collect()\n",
    "        print(\"Epoch\", epoch, \"completed\")\n",
    "        print(\"Elapsed time:\", (time.time() - start_time)/60.0, \"minutes\" )\n",
    "        run_history.append(epoch_history)\n",
    "    \n",
    "    print()\n",
    "    print(\"Saving run\", run, \"loss etc.\")\n",
    "    history_name = \"ds_\"+str(ds)+\"_run_\" + str(run) +\"_histories.pkl\"\n",
    "\n",
    "    with open(history_name, 'wb') as file_pi:\n",
    "        pickle.dump(run_history, file_pi)\n",
    "    \n",
    "    model_weights_name = \"ds_\"+str(ds)+\"_run_\" + str(run) +\"_model_weights.h5\"\n",
    "    \n",
    "    print(\"Saving run\", run, \"model weights as\", model_weights_name)\n",
    "    model.save_weights(model_weights_name)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "print(\"Total time:\", (time.time() - start_time)/60.0, \"minutes\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print( run_history[0].history )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
